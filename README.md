# ğŸ§¹ ETL Project: Pandas to Snowflakeâ„ï¸

![ETL Pandas to Snowflake Thumbnail](images/thumbnail/thumbnail_image.png)


This project is all about building a full ETL (Extract, Transform, Load) pipeline using Python and Snowflake. It helps take messy raw data, clean it using pandas, and load it into a Snowflake table â€” all step-by-step with interactive user prompts ğŸ§‘â€ğŸ’»âš™ï¸â„ï¸

---

## ğŸš¦ Whatâ€™s the Project About?

- Youâ€™ll start with **raw messy data** files â€” 3 of them: customers, orders, and products ğŸ“„ğŸ“¦ğŸ§¾  
- Then, the data goes through a **cleaning process** using pandas. We handle nulls, rename columns, and fix any inconsistencies ğŸ”ğŸ§½  
- Next, the cleaned data from all 3 tables is saved into a single CSV file âœ…  
- Finally, that combined cleaned file is read and pushed into **Snowflake**, where it gets stored in a proper table ğŸ§ŠğŸ“¥

---

## ğŸ—‚ï¸ Folders and Their Purpose

- **`raw_data/`**  
  Contains the 3 messy CSV files: `customers_messy.csv`, `orders_messy.csv`, and `products_messy.csv`. These are the starting point of our ETL pipeline.

- **`cleaned_data/`**  
  Inside this, youâ€™ll find `cleaned_data.csv`, which is an **empty file at first**. After processing, it will contain the final cleaned version of your data ğŸ§¼ğŸ“

- **`screenshots_etl/`**  
  Contains before and after cleaning screenshots for all 3 tables (customers, orders, products). You can visually see how the data looked before vs after applying cleaning logic ğŸ–¼ï¸âœ¨

- **`screenshots_snowflake/`**  
  Shows Snowflake database and schema:
  - Before loading: just DB and schema (no table âŒ)
  - After loading: DB, schema, and the newly created table (âœ…)

- **`terminal_output_project/`**  
  Has two continuous terminal screenshots. They show the full output while running the ETL script â€” from â€œETL startedâ€ to â€œETL completed successfullyâ€ ğŸ”ğŸ“¸

---

## ğŸ¯ How Does It Work?

The whole flow is user-friendly and prompt-based:

1. First, the script will ask for your **Snowflake credentials** to connect safely (donâ€™t worry, credentials are hidden in terminal ğŸ”’)
2. Then, it asks you to give the path of raw data files â€” just go into the `raw_data/` folder and copy-paste the paths ğŸ”—
3. The script will then clean the data from all 3 files using pandas ğŸ¼ğŸ§¹
4. It saves the final cleaned data into the `cleaned_data.csv` file (initially empty) ğŸ“„
5. Then, that cleaned file is automatically read again ğŸ“– and uploaded into your Snowflake table ğŸ§ŠğŸ“¤
6. Now just head over to Snowflake and check â€” your table will be there, all clean and ready ğŸš€

---

## ğŸ‰ ETL Completed Successfully!

Thatâ€™s a wrap. From start to finish, everything is handled step-by-step â€” cleaning, saving, and uploading.

ğŸ”¥ You just need to **clone the project**, run the script, and the pipeline will guide you through the rest.

